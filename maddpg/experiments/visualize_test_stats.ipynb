{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "BLUE = [0, 0.4470, 0.7410]\n",
    "RED = [0.8500, 0.3250, 0.0980]\n",
    "YELLOW = [0.929, 0.6940, 0.1250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "This code loads the data as a list 'trajectories' and also makes a data frame 'df' for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'fully-connected_03'\n",
    "\n",
    "def load_data(run_name):\n",
    "    # import pickle file from ./test_policy/test_trajectory.pkl\n",
    "    with open('./test_policy/test_'+run_name+'/test_trajectory.pkl', 'rb') as f:\n",
    "        trajectories = pickle.load(f)\n",
    "\n",
    "    # calculate cumulative reward for each agent until each timestep as additional columns\n",
    "    for trajectory in trajectories:\n",
    "        for i in range(len(trajectory)):\n",
    "            trajectory[i].extend([sum([row[14] for row in trajectory[:i+1]]), sum([row[15] for row in trajectory[:i+1]]), sum([row[16] for row in trajectory[:i+1]])])\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        for k in range(len(trajectory)):\n",
    "            dists = []\n",
    "            for i in range(3):\n",
    "                dist = []\n",
    "                for j in range(3):\n",
    "                    dist.append(np.linalg.norm(np.array(trajectory[k][2+2*i:4+2*i]) - np.array(trajectory[k][8+2*j:10+2*j])))\n",
    "                dists.append(min(dist))\n",
    "            trajectory[k].extend(dists)\n",
    "\n",
    "    # trajectories is a list of trajectories. Where each trajectory is a list of:\n",
    "    # [timestep, agent1_x, agent1_y, agent2_x, agent2_y, agent3_x, agent3_y, landmark1_x, landmark1_y, landmark2_x, landmark2_y, landmark3_x, landmark3_y, reward_1, reward_2, reward_3]\n",
    "    # convert trajectories to pd dataframe with columns: episode, timestep, agent1_x, agent1_y, agent2_x, agent2_y, agent3_x, agent3_y, reward_1, reward_2, reward_3\n",
    "    # Flatten the nested list\n",
    "    flattened_data = [tup for trajectory in trajectories for tup in trajectory]\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flattened_data, columns=[ 'episode', 'timestep',\n",
    "                                                'agent1_x', 'agent1_y',\n",
    "                                                'agent2_x', 'agent2_y', \n",
    "                                                'agent3_x', 'agent3_y', \n",
    "                                                'land1_x', 'land1_y',\n",
    "                                                'land2_x', 'land2_y', \n",
    "                                                'land3_x', 'land3_y', \n",
    "                                                'reward_1', 'reward_2', 'reward_3',\n",
    "                                                'agent1_cum_reward', 'agent2_cum_reward', 'agent3_cum_reward',\n",
    "                                                'agent1_dist', 'agent2_dist', 'agent3_dist'])\n",
    "    return df, trajectories\n",
    "\n",
    "df, trajectories = load_data(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Performance Analysis\n",
    "This code runs analysis on all the test episodes wihin the loaded run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_analysis(df, run_name):\n",
    "    %matplotlib qt\n",
    "    # Colors\n",
    "    BLUE = [0, 0.4470, 0.7410]\n",
    "    RED = [0.8500, 0.3250, 0.0980]\n",
    "    YELLOW = [0.929, 0.6940, 0.1250]\n",
    "\n",
    "    p = [BLUE, RED, YELLOW]\n",
    "    sns.set_palette(p)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2,3, figsize=(15,8))\n",
    "\n",
    "    fig.suptitle('Policy used: {}'.format(run_name), fontsize=16)\n",
    "\n",
    "\n",
    "    sns.lineplot(ax=axs[0,0], x=\"timestep\", y=\"agent1_dist\", data=df, errorbar=(\"sd\",1), sort=False, color=BLUE, label='agent 1')\n",
    "    sns.lineplot(ax=axs[0,0], x=\"timestep\", y=\"agent2_dist\", data=df, errorbar=(\"sd\",1), sort=False, color=RED, label='agent 2')\n",
    "    sns.lineplot(ax=axs[0,0], x=\"timestep\", y=\"agent3_dist\", data=df, errorbar=(\"sd\",1), sort=False, color=YELLOW, label='agent 3')\n",
    "    axs[0,0].set_xlabel('timestep')\n",
    "    axs[0,0].set_ylabel('distance to closest landmark')\n",
    "    axs[0,0].grid()\n",
    "\n",
    "    # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "    # sns.violinplot(data=df, x=\"timestep\", y=\"reward_1\")\n",
    "    melted_df = data=df[df['timestep']==70].melt(value_vars=['agent1_cum_reward', 'agent2_cum_reward', 'agent3_cum_reward'], var_name='agent_name', value_name='value')\n",
    "    sns.histplot(ax=axs[0,1], data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=30)\n",
    "    axs[0,1].set_xlabel('cumulative shared reward')\n",
    "    axs[0,1].set_ylabel('number of episodes')\n",
    "    axs[0,1].legend(['agent 1', 'agent 2', 'agent 3'])\n",
    "\n",
    "\n",
    "    sns.lineplot(ax=axs[0,2], x=\"timestep\", y=\"agent1_cum_reward\", data=df, errorbar=(\"sd\",1), sort=False, color=BLUE, label='agent 1')\n",
    "    sns.lineplot(ax=axs[0,2], x=\"timestep\", y=\"agent2_cum_reward\", data=df, errorbar=(\"sd\",1), sort=False, color=RED, label='agent 2')\n",
    "    sns.lineplot(ax=axs[0,2], x=\"timestep\", y=\"agent3_cum_reward\", data=df, errorbar=(\"sd\",1), sort=False, color=YELLOW, label='agent 3')\n",
    "    axs[0,2].set_xlabel('timestep')\n",
    "    axs[0,2].set_ylabel('cumulative shared reward')\n",
    "    axs[0,2].grid()\n",
    "\n",
    "    # plt.subplot(2,2,4)\n",
    "    sns.lineplot(ax=axs[1,0], x=\"timestep\", y=\"reward_1\", data=df, errorbar=(\"sd\",1), sort=False, color=BLUE, label='agent 1')\n",
    "    sns.lineplot(ax=axs[1,0], x=\"timestep\", y=\"reward_2\", data=df, errorbar=(\"sd\",1), sort=False, color=RED, label='agent 2')\n",
    "    sns.lineplot(ax=axs[1,0], x=\"timestep\", y=\"reward_3\", data=df, errorbar=(\"sd\",1), sort=False, color=YELLOW, label='agent 3')\n",
    "    axs[1,0].set_xlabel('timestep')\n",
    "    axs[1,0].set_ylabel('mean shared reward')\n",
    "    axs[1,0].grid()\n",
    "\n",
    "\n",
    "    # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "    melted_df = data=df[df['timestep']==1].melt(value_vars=['agent1_dist', 'agent2_dist', 'agent3_dist'], var_name='agent_name', value_name='value')\n",
    "    sns.histplot(ax=axs[1,1],data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=30)\n",
    "    axs[1,1].set_xlabel('distance to closest landmark at the first timestep')\n",
    "    axs[1,1].set_ylabel('number of episodes')\n",
    "    axs[1,1].legend(['agent 1', 'agent 2', 'agent 3'])\n",
    "\n",
    "\n",
    "    # plot the histogram of the mean of the distance to the closest landmark for each agent at the timestep 70\n",
    "    melted_df = data=df[df['timestep']==70].melt(value_vars=['agent1_dist', 'agent2_dist', 'agent3_dist'], var_name='agent_name', value_name='value')\n",
    "    sns.histplot(ax=axs[1,2], data=melted_df, x='value', hue='agent_name', element='step', common_norm=False, bins=30)\n",
    "    axs[1,2].set_xlabel('distance to closest landmark at the last timestep')\n",
    "    axs[1,2].set_ylabel('number of episodes')\n",
    "    axs[1,2].legend(['agent 1', 'agent 2', 'agent 3'])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # fig.savefig(run_name+'_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_test_analysis(df, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Analysis\n",
    "This code plots the details of a particular episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "episode = 123\n",
    "\n",
    "plt.figure(figsize=(15,4.5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "# plot the x,y trajectory of agents in episode 1 sorted by timestep using seaborn\n",
    "sns.lineplot(x=\"agent1_x\", y=\"agent1_y\", data=df[df['episode']==episode], sort=False, color=BLUE)\n",
    "sns.lineplot(x=\"agent2_x\", y=\"agent2_y\", data=df[df['episode']==episode], sort=False, color=RED)\n",
    "sns.lineplot(x=\"agent3_x\", y=\"agent3_y\", data=df[df['episode']==episode], sort=False, color=YELLOW)\n",
    "# mark the location of landmarks\n",
    "sns.scatterplot(x=\"land1_x\", y=\"land1_y\", data=df[df['episode']==episode], color='gray', s=300)\n",
    "sns.scatterplot(x=\"land2_x\", y=\"land2_y\", data=df[df['episode']==episode], color='gray', s=300)\n",
    "sns.scatterplot(x=\"land3_x\", y=\"land3_y\", data=df[df['episode']==episode], color='gray', s=300)\n",
    "# make axis equal and limit the axis to -1.0 to 1.0\n",
    "plt.axis('square')\n",
    "plt.xlim(-1.5,1.5)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "# plot the reward of agents in episode 1 sorted by timestep using seaborn\n",
    "sns.lineplot(x=\"timestep\", y=\"reward_1\", data=df[df['episode']==episode], sort=False, color=BLUE, label='agent1')\n",
    "sns.lineplot(x=\"timestep\", y=\"reward_2\", data=df[df['episode']==episode], sort=False, color=RED, label='agent2')\n",
    "sns.lineplot(x=\"timestep\", y=\"reward_3\", data=df[df['episode']==episode], sort=False, color=YELLOW, label='agent3')\n",
    "plt.xlabel('timestep')\n",
    "plt.ylabel('shared reward')\n",
    "plt.title('Episode {}'.format(episode))\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# plot the distance to the closest landmark for each agent\n",
    "sns.lineplot(x=\"timestep\", y=\"agent1_dist\", data=df[df['episode']==episode], sort=False, color=BLUE, label='agent1')\n",
    "sns.lineplot(x=\"timestep\", y=\"agent2_dist\", data=df[df['episode']==episode], sort=False, color=RED, label='agent2')\n",
    "sns.lineplot(x=\"timestep\", y=\"agent3_dist\", data=df[df['episode']==episode], sort=False, color=YELLOW, label='agent3')\n",
    "plt.xlabel('timestep')\n",
    "plt.ylabel('distance to closest landmark')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(d):\n",
    "        return np.exp(-(d**2)/.1)\n",
    "\n",
    "def plot_reward_function(df):\n",
    "\n",
    "    # get the landmark locations for episode 1 timestep 1 from df\n",
    "    landmark_locations = df[(df['timestep']==1) & (df['episode']==1)][['land1_x', 'land1_y', 'land2_x', 'land2_y', 'land3_x', 'land3_y']]\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    y = np.linspace(-1, 1, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    lanmark_locations = [(landmark_locations.land1_x.values[0], landmark_locations.land1_y.values[0]), \n",
    "                         (landmark_locations.land2_x.values[0], landmark_locations.land2_y.values[0]), \n",
    "                         (landmark_locations.land3_x.values[0], landmark_locations.land3_y.values[0])]\n",
    "\n",
    "    # calculate the distance to the closest landmark for each point in the grid\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Y)):\n",
    "            dists = []\n",
    "            for k in range(3):\n",
    "                dists.append(np.linalg.norm(np.array([X[i,j], Y[i,j]]) - np.array(lanmark_locations[k])))\n",
    "            Z[i,j] = reward_function(min(dists))\n",
    "    # plot the reward function as a 2D heatmap using sns\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Reward Function')\n",
    "    cs = ax.contourf(X, Y, Z, 50, cmap='cividis')\n",
    "    cbar = fig.colorbar(cs)\n",
    "    # limit the colorbar to 0 to 1 and make sure 0 and 1 are part of the ticks\n",
    "    cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    # plot contour lines with labels on top of the heatmap\n",
    "    cs = ax.contour(X, Y, Z, [0.01,0.1,0.25,0.5,0.75,0.95], colors='white', linewidths=0.5)\n",
    "    # add specific contour values\n",
    "    ax.clabel(cs, inline=1, fontsize=8)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.show()\n",
    "plot_reward_function(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the episode number of the wrost performing episodes and see the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>timestep</th>\n",
       "      <th>agent1_x</th>\n",
       "      <th>agent1_y</th>\n",
       "      <th>agent2_x</th>\n",
       "      <th>agent2_y</th>\n",
       "      <th>agent3_x</th>\n",
       "      <th>agent3_y</th>\n",
       "      <th>land1_x</th>\n",
       "      <th>land1_y</th>\n",
       "      <th>...</th>\n",
       "      <th>land3_y</th>\n",
       "      <th>reward_1</th>\n",
       "      <th>reward_2</th>\n",
       "      <th>reward_3</th>\n",
       "      <th>agent1_cum_reward</th>\n",
       "      <th>agent2_cum_reward</th>\n",
       "      <th>agent3_cum_reward</th>\n",
       "      <th>agent1_dist</th>\n",
       "      <th>agent2_dist</th>\n",
       "      <th>agent3_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58379</th>\n",
       "      <td>834</td>\n",
       "      <td>70</td>\n",
       "      <td>3.719370</td>\n",
       "      <td>-2.282283</td>\n",
       "      <td>-0.229380</td>\n",
       "      <td>0.396079</td>\n",
       "      <td>0.719549</td>\n",
       "      <td>0.033549</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>1.395175e-68</td>\n",
       "      <td>1.395175e-68</td>\n",
       "      <td>1.395175e-68</td>\n",
       "      <td>2.300226e-10</td>\n",
       "      <td>2.300226e-10</td>\n",
       "      <td>2.300226e-10</td>\n",
       "      <td>3.946284</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.222097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>4.470279</td>\n",
       "      <td>-1.845780</td>\n",
       "      <td>7.713436</td>\n",
       "      <td>-0.558476</td>\n",
       "      <td>-1.034868</td>\n",
       "      <td>-1.584986</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>9.401575e-320</td>\n",
       "      <td>9.401575e-320</td>\n",
       "      <td>9.401575e-320</td>\n",
       "      <td>6.666467e-10</td>\n",
       "      <td>6.666467e-10</td>\n",
       "      <td>6.666467e-10</td>\n",
       "      <td>4.378358</td>\n",
       "      <td>7.235023</td>\n",
       "      <td>1.393937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14209</th>\n",
       "      <td>203</td>\n",
       "      <td>70</td>\n",
       "      <td>4.072829</td>\n",
       "      <td>0.775028</td>\n",
       "      <td>6.683566</td>\n",
       "      <td>-1.780517</td>\n",
       "      <td>0.915880</td>\n",
       "      <td>-0.981435</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>1.554979e-243</td>\n",
       "      <td>1.554979e-243</td>\n",
       "      <td>1.554979e-243</td>\n",
       "      <td>7.240644e-10</td>\n",
       "      <td>7.240644e-10</td>\n",
       "      <td>7.240644e-10</td>\n",
       "      <td>3.655923</td>\n",
       "      <td>6.434806</td>\n",
       "      <td>1.065913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47529</th>\n",
       "      <td>679</td>\n",
       "      <td>70</td>\n",
       "      <td>4.398247</td>\n",
       "      <td>0.471654</td>\n",
       "      <td>6.204931</td>\n",
       "      <td>5.244140</td>\n",
       "      <td>0.940527</td>\n",
       "      <td>-0.545348</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.760432e-09</td>\n",
       "      <td>1.760432e-09</td>\n",
       "      <td>1.760432e-09</td>\n",
       "      <td>3.926676</td>\n",
       "      <td>7.749016</td>\n",
       "      <td>0.701048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>2.448700</td>\n",
       "      <td>-0.335249</td>\n",
       "      <td>1.154482</td>\n",
       "      <td>2.644794</td>\n",
       "      <td>0.475252</td>\n",
       "      <td>-0.664066</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>1.948412e-49</td>\n",
       "      <td>1.948412e-49</td>\n",
       "      <td>1.948412e-49</td>\n",
       "      <td>3.097751e-09</td>\n",
       "      <td>3.097751e-09</td>\n",
       "      <td>3.097751e-09</td>\n",
       "      <td>1.977328</td>\n",
       "      <td>2.620028</td>\n",
       "      <td>0.664527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode  timestep  agent1_x  agent1_y  agent2_x  agent2_y  agent3_x  \\\n",
       "58379      834        70  3.719370 -2.282283 -0.229380  0.396079  0.719549   \n",
       "5319        76        70  4.470279 -1.845780  7.713436 -0.558476 -1.034868   \n",
       "14209      203        70  4.072829  0.775028  6.683566 -1.780517  0.915880   \n",
       "47529      679        70  4.398247  0.471654  6.204931  5.244140  0.940527   \n",
       "5039        72        70  2.448700 -0.335249  1.154482  2.644794  0.475252   \n",
       "\n",
       "       agent3_y  land1_x  land1_y  ...   land3_y       reward_1  \\\n",
       "58379  0.033549      0.5      0.0  ... -0.433013   1.395175e-68   \n",
       "5319  -1.584986      0.5      0.0  ... -0.433013  9.401575e-320   \n",
       "14209 -0.981435      0.5      0.0  ... -0.433013  1.554979e-243   \n",
       "47529 -0.545348      0.5      0.0  ... -0.433013   0.000000e+00   \n",
       "5039  -0.664066      0.5      0.0  ... -0.433013   1.948412e-49   \n",
       "\n",
       "            reward_2       reward_3  agent1_cum_reward  agent2_cum_reward  \\\n",
       "58379   1.395175e-68   1.395175e-68       2.300226e-10       2.300226e-10   \n",
       "5319   9.401575e-320  9.401575e-320       6.666467e-10       6.666467e-10   \n",
       "14209  1.554979e-243  1.554979e-243       7.240644e-10       7.240644e-10   \n",
       "47529   0.000000e+00   0.000000e+00       1.760432e-09       1.760432e-09   \n",
       "5039    1.948412e-49   1.948412e-49       3.097751e-09       3.097751e-09   \n",
       "\n",
       "       agent3_cum_reward  agent1_dist  agent2_dist  agent3_dist  \n",
       "58379       2.300226e-10     3.946284     0.042300     0.222097  \n",
       "5319        6.666467e-10     4.378358     7.235023     1.393937  \n",
       "14209       7.240644e-10     3.655923     6.434806     1.065913  \n",
       "47529       1.760432e-09     3.926676     7.749016     0.701048  \n",
       "5039        3.097751e-09     1.977328     2.620028     0.664527  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the episodes with the worst cumulative reward at the timestep 70\n",
    "df[df['timestep']==70].sort_values(by='agent1_cum_reward', ascending=True).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Reply Animation\n",
    "\n",
    "Just adjust the episode number and observe the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import pygame\n",
    "\n",
    "episode = 123\n",
    "# Sample trajectory data\n",
    "trajectory_ = trajectories[episode]\n",
    "# drop the first element of the list for each list item in trajectory\n",
    "trajectory_ = [t[1:] for t in trajectory_]\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "# Display settings\n",
    "WIDTH, HEIGHT = 640, 480\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Multi-Agent and Landmarks Animation\")\n",
    "\n",
    "# Colors\n",
    "BLUE = np.array([0, 0.4470*255, 0.7410*255])\n",
    "RED = np.array([0.8500*255, 0.3250*255, 0.0980*255])\n",
    "YELLOW = np.array([0.929*255, 0.6940*255, 0.1250*255])\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "AGENTS_COLORS = [BLUE, RED, YELLOW]\n",
    "LANDMARK_COLORS = [(50, 50, 50), (50, 50, 50), (50, 50, 50)]\n",
    "BLACK = (20, 20, 20)\n",
    "\n",
    "frames = []\n",
    "# shift and scale the data (-1,1) to fit the screen size (0,500) and (0,500)\n",
    "SCALE_FACTOR = 200  # given our dimensions and trajectory range\n",
    "SCREEN_CENTER = (WIDTH // 2, HEIGHT // 2)\n",
    "\n",
    "# Choose a font (using a default system font here)\n",
    "font = pygame.font.SysFont(\"arial\", 16)\n",
    "\n",
    "def map_to_screen(pos):\n",
    "    \"\"\"Map a trajectory position to a screen position.\"\"\"\n",
    "    return int(pos[0] * SCALE_FACTOR + SCREEN_CENTER[0]), int(pos[1] * SCALE_FACTOR + SCREEN_CENTER[1])\n",
    "\n",
    "\n",
    "def draw_entity(screen, x, y, color, size=.2*SCALE_FACTOR):\n",
    "    pygame.draw.circle(screen, color, (int(x), int(y)), size)\n",
    "\n",
    "\n",
    "\n",
    "def display_text(text, x, y, color=BLACK):\n",
    "    \"\"\"Render and display text on the screen at specified coordinates.\"\"\"\n",
    "    text_surface = font.render(text, True, color)\n",
    "    screen.blit(text_surface, (x, y))\n",
    "\n",
    "def capture_frame(screen):\n",
    "    \"\"\"Capture the current Pygame screen frame.\"\"\"\n",
    "    frame = pygame.Surface(screen.get_size())\n",
    "    frame.blit(screen, (0, 0))\n",
    "    frames.append(frame)\n",
    "\n",
    "\n",
    "def save_frames_to_video(frames, filename, fps=30):\n",
    "    \"\"\"Save captured frames to a video file.\"\"\"\n",
    "    height, width = frames[0].get_size()\n",
    "    size = (width, height)\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'XVID'), fps, size)\n",
    "    for frame in frames:\n",
    "        # Convert Pygame surface to OpenCV format\n",
    "        frame_rgb = pygame.surfarray.array3d(frame).transpose([1, 0, 2])\n",
    "        frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "def main():\n",
    "    time.sleep(5)\n",
    "    clock = pygame.time.Clock()\n",
    "    running = True\n",
    "    current_time = 0\n",
    "    while running:\n",
    "        screen.fill(WHITE)\n",
    "        # Calculate the current positions of the agents and landmarks\n",
    "        for i in range(len(trajectory_) - 1):\n",
    "\n",
    "            cum_rew_1 = trajectory_[i][16]\n",
    "            cum_rew_2 = trajectory_[i][17]\n",
    "            cum_rew_3 = trajectory_[i][18]\n",
    "\n",
    "            t0, *data0 = trajectory_[i]\n",
    "            t1, *data1 = trajectory_[i + 1]\n",
    "\n",
    "            if t0 <= current_time < t1:\n",
    "                alpha = (current_time - t0) / (t1 - t0)\n",
    "\n",
    "                # Drawing agents\n",
    "                for j in range(3):  \n",
    "                    x0, y0, x1, y1 = data0[j * 2], data0[j * 2 + 1], data1[j * 2], data1[j * 2 + 1]\n",
    "                    # r0, r1 = data0[12 + j], data1[12 + j]\n",
    "                    x = x0 * (1 - alpha) + x1 * alpha\n",
    "                    y = y0 * (1 - alpha) + y1 * alpha\n",
    "                    r = data0[12 + j]\n",
    "                    screen_x, screen_y = map_to_screen((x, y))\n",
    "                    draw_entity(screen, screen_x, screen_y, AGENTS_COLORS[j])\n",
    "                    display_text(f\"{r:.4f}\", screen_x-25, screen_y-60)\n",
    "\n",
    "                # Drawing landmarks\n",
    "                for j in range(3):\n",
    "                    x0, y0, x1, y1 = data0[6 + j * 2], data0[7 + j * 2], data1[6 + j * 2], data1[7 + j * 2]\n",
    "                    x = x0 * (1 - alpha) + x1 * alpha\n",
    "                    y = y0 * (1 - alpha) + y1 * alpha\n",
    "                    screen_x, screen_y = map_to_screen((x, y))\n",
    "                    draw_entity(screen, screen_x, screen_y, LANDMARK_COLORS[j], size=.05*SCALE_FACTOR)\n",
    "\n",
    "                display_text(f\"agent 1 cumulative reward: {cum_rew_1:.2f}\", 370, 10)\n",
    "                display_text(f\"agent 2 cumulative reward: {cum_rew_2:.2f}\", 370, 30)\n",
    "                display_text(f\"agent 2 cumulative reward: {cum_rew_3:.2f}\", 370, 50)\n",
    "                display_text(f\"episode: {episode}\",10, 20)\n",
    "                display_text(f\"timestep: {i}\", 120, 20)\n",
    "\n",
    "        pygame.display.flip()   \n",
    "        capture_frame(screen)\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        \n",
    "        clock.tick(60)\n",
    "        # Update current time\n",
    "        current_time += .1\n",
    "\n",
    "        # Exit loop when trajectory ends\n",
    "        if current_time > trajectory[-1][0]:\n",
    "            running = False\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # save_frames_to_video(frames,'episode_{}.avi'.format(episode))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
